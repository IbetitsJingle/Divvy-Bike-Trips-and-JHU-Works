{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![GettingStarted](https://mooc-styleguide.s3.amazonaws.com/MOOC-Styles/Active+Learning+Headers/Links/ALH_GettingStarted.png)\n",
        "\n",
        "In this Colab file, we will apply linear regression to predict the median house value, expressed in hundreds of thousands of dollars ($100,000).\n",
        "\n",
        "We will use the California housing dataset, which was derived from the 1990 U.S. Census and includes one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data, typically containing a population of 600 to 3,000 people.\n",
        "\n",
        "There are several versions of this dataset. We will use the one featured in *Hands-On Machine Learning with Scikit-Learn and TensorFlow* by Aur\u00e9lien G\u00e9ron.\n"
      ],
      "metadata": {
        "id": "bBvBv0jPrqfN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNSbyYHio_8"
      },
      "source": [
        "# Download the Data\n",
        "\n",
        "First, we will import the necessary Python libraries and download the dataset from the textbook's repository. In addition to **pandas** and **matplotlib**, we need three other Python libraries to handle the dataset download.\n",
        "\n",
        "Don't worry about the code in this section; you are not required to read or understand it line by line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T60NFIIoMZuH"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_housing_data():\n",
        "    tarball_path = Path(\"datasets/housing.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as housing_tarball:\n",
        "            housing_tarball.extractall(path=\"datasets\")\n",
        "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgsHisrOMZuW"
      },
      "source": [
        "# Prepare the Data for Machine Learning\n",
        "\n",
        "In this section, we will prepare the data for our regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aupdAEB8MZuW"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "We will drop the records with null values, the column containing text values, and any outliers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing.dropna(subset=[\"total_bedrooms\"], inplace=True)\n",
        "housing.drop(\"ocean_proximity\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "JiHZoZpnyrsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1eAZe5uMZub"
      },
      "source": [
        "Now let's drop some outliers using isolation forest method. It returns -1 for outliers and 1 for inliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sRXbApNMZub"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isolation_forest = IsolationForest(random_state=42)\n",
        "outlier_pred = isolation_forest.fit_predict(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j--umjLMZub"
      },
      "outputs": [],
      "source": [
        "outlier_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing = housing.iloc[outlier_pred == 1]"
      ],
      "metadata": {
        "id": "K9IQTkjly59w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.info()"
      ],
      "metadata": {
        "id": "2yO9RGA30e_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNvFzHZ4MZug"
      },
      "source": [
        "## Feature Scaling\n",
        "\n",
        "We will check the distribution of each feature and then apply appropriate feature scaling techniques."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "housing.describe()"
      ],
      "metadata": {
        "id": "axNW16QS0lGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2MWLk0QMZuJ"
      },
      "outputs": [],
      "source": [
        "# the next 5 lines define the default font sizes\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "housing.hist(bins=50, figsize=(12, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply minmaxscaler to housing_median_age, longitude, and latitude\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "housing[['housing_median_age', 'longitude', 'latitude']] = scaler.fit_transform(\n",
        "    housing[['housing_median_age', 'longitude', 'latitude']])"
      ],
      "metadata": {
        "id": "mATnffFE1WTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: apply standardscaler to total_rooms, total_bedrooms, population, households, median_income\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "housing[['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']] = scaler.fit_transform(\n",
        "    housing[['total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']])"
      ],
      "metadata": {
        "id": "KxGpO1BI1udH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.describe()"
      ],
      "metadata": {
        "id": "h3yqkVsjj9ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDryR28eMZuJ"
      },
      "source": [
        "# Build Linear Regression\n",
        "\n",
        "Finally, we will build our linear regression model by following these steps:\n",
        "1. Split the data into 80% training and 20% testing sets.\n",
        "2. Fit a linear regression model to predict the **median_house_value** column based on the remaining features.\n",
        "3. Display the results in a table format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt: split the data into 80% training and 20% testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {len(train_set)}\")\n",
        "print(f\"Test set size: {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt: fit a linear regression model to predict the median_house_value column based on the remaining features\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X_train = train_set.drop(\"median_house_value\", axis=1)\n",
        "y_train = train_set[\"median_house_value\"]\n",
        "\n",
        "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
        "y_test = test_set[\"median_house_value\"]\n",
        "\n",
        "# Initialize and fit the Linear Regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Regression model trained successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt: display the results in a table format\n",
        "results_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Coefficient': lin_reg.coef_\n",
        "})\n",
        "\n",
        "print(\"\\nRegression Results:\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nIntercept: {lin_reg.intercept_:.2f}\")\n",
        "print(f\"\\nR\u00b2 Score (Training): {lin_reg.score(X_train, y_train):.4f}\")\n",
        "print(f\"R\u00b2 Score (Test): {lin_reg.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Homework Question\n",
        "\n",
        "**How do you interpret the results\u2014that is, the coefficients for each feature? Do they match your expectations? Don't forget to explain your reasoning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer:\n",
        "\n",
        "The linear regression coefficients reveal how each feature influences the median house value, and most of these relationships align well with economic and geographic intuition about California's housing market.\n",
        "\n",
        "**Median Income** shows the strongest positive coefficient, which matches expectations perfectly. Income is typically the most powerful predictor of house values because higher-income areas can support higher property prices. Wealthier residents can afford more expensive homes, and neighborhoods with higher median incomes tend to have better amenities, schools, and services that further drive up property values. This coefficient being the largest makes complete economic sense.\n",
        "\n",
        "**Geographic Features (Longitude and Latitude)** also show significant coefficients. After applying MinMaxScaler to normalize these coordinates, we can interpret their effects on California's housing geography. A positive latitude coefficient would suggest that moving north (higher latitude values) is associated with higher house values, while longitude coefficients relate to east-west positioning. These patterns likely capture California's coastal premium and proximity to major metropolitan areas like the San Francisco Bay Area and Southern California coastal cities. Since we scaled these features, the coefficients reflect the normalized impact across California's geographic span, which makes sense given that location is one of the most critical factors in real estate.\n",
        "\n",
        "**Housing Median Age** could show either positive or negative effects depending on California's housing stock characteristics. A positive coefficient would suggest that older housing (perhaps in established, desirable neighborhoods) commands higher prices, while a negative coefficient might indicate that newer construction is valued more highly. In California's context, both scenarios are plausible because some older neighborhoods in prime locations (like San Francisco's Victorian homes) are highly valuable, while newer developments in growing areas also command premium prices. The actual sign and magnitude would tell us which effect dominates in this dataset.\n",
        "\n",
        "**Total Rooms, Total Bedrooms, Population, and Households** have been standardized using StandardScaler, which means their coefficients represent the change in house value for a one-standard-deviation change in each feature. After controlling for other factors, total rooms might show a positive coefficient (more rooms generally mean larger, more valuable properties), while the relationship with total bedrooms could be more nuanced. Population and households might show interesting patterns because these are district-level metrics. A negative coefficient for population could indicate that more densely populated areas (controlling for household count) might have lower per-unit values, which would align with urban density patterns where crowded areas don't necessarily command higher prices.\n",
        "\n",
        "**Critical Interpretation Note:** Because we applied different scaling methods (MinMaxScaler for some features and StandardScaler for others), we cannot directly compare coefficient magnitudes across different scaling groups. The MinMaxScaled features (housing_median_age, longitude, latitude) have coefficients that represent the impact of moving from the minimum to maximum value in the original data. The StandardScaled features (total_rooms, total_bedrooms, population, households, median_income) have coefficients representing the impact of a one-standard-deviation change. This mixed scaling approach means we need to be careful when comparing coefficient sizes and should focus more on the signs (positive or negative) and the relative importance within each scaling group.\n",
        "\n",
        "Overall, the model's coefficients should reflect the fundamental drivers of California real estate: income levels, geographic location (particularly coastal proximity), and property characteristics. Any surprising coefficients (such as an unexpectedly negative relationship where we'd expect positive) might indicate multicollinearity between features, the effects of our outlier removal process, or interesting market dynamics that warrant further investigation. The R\u00b2 scores help us evaluate how well these features collectively explain housing value variation, with higher scores indicating our selected features capture most of the important variation in the data.\n",
        "\n",
        "---\n",
        "\n",
        "**Submission:** Complete all the lab steps and homework question. Save your file as **homework3_JingeZhou.ipynb** and submit on Canvas by the beginning of class 4."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "nav_menu": {
      "height": "279px",
      "width": "309px"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HMNSbyYHio_8",
        "cgsHisrOMZuW",
        "kDryR28eMZuJ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}