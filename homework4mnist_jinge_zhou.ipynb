{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opn3KTb3Ylk1"
      },
      "source": [
        "# BU.330.775 Machine Learning: Design and Deployment\n",
        "## Lab 4. Model evaluation on MNIST dataset\n",
        "Jinge Zhou\n",
        "\n",
        "**Learning Goal:** Evaluate multiple supervised machine learning approaches on the MNIST database\n",
        "\n",
        "**Background:** The MNIST dataset consists of handwritten digits with 784 features. This dataset was contributed by Yann LeCun, Corinna Cortes, and Christopher J.C. Burges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF8O2ZwBYlk2"
      },
      "source": [
        "## Setup: Import Libraries and Set Default Plot Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z1awpwlYlk2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set up default font sizes for plots\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w7jvmEEYlk2"
      },
      "source": [
        "## Step 1: Load and Explore the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gpKGEtJYlk2"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', as_frame=False)\n",
        "X, y = mnist.data, mnist.target\n",
        "\n",
        "print(\"Shape of X (features):\", X.shape)\n",
        "print(\"Shape of y (labels):\", y.shape)\n",
        "print(\"\\nFirst 10 labels:\", y[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06o7hSh-Ylk2"
      },
      "source": [
        "## Step 2: Visualize Sample Digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOfVjGLQYlk3"
      },
      "outputs": [],
      "source": [
        "def plot_digit(image_data):\n",
        "    \"\"\"Display a single digit image.\"\"\"\n",
        "    image = image_data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap=\"binary\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Display first 100 images in a 10x10 grid\n",
        "plt.figure(figsize=(9, 9))\n",
        "for idx, image_data in enumerate(X[:100]):\n",
        "    plt.subplot(10, 10, idx + 1)\n",
        "    plot_digit(image_data)\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNOAKuRiYlk3"
      },
      "source": [
        "## Step 3: Split Data into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4FVsOOWYlk3"
      },
      "outputs": [],
      "source": [
        "# Use first 60,000 images for training, remaining 10,000 for testing\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
        "\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Testing set size:\", X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyZ9kP33Ylk4"
      },
      "source": [
        "## Step 4: Train Binary Classifier (Digit 2 vs Not 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaGe_s1KYlk4"
      },
      "outputs": [],
      "source": [
        "# Create binary labels: True if digit is 2, False otherwise\n",
        "y_train_2 = (y_train == '2')\n",
        "y_test_2 = (y_test == '2')\n",
        "\n",
        "# Train SGD Classifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "sgd_clf = SGDClassifier(random_state=46)\n",
        "sgd_clf.fit(X_train, y_train_2)\n",
        "\n",
        "print(\"Binary classifier trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq7sk2lLYlk5"
      },
      "source": [
        "## Step 5: Evaluate Binary Classifier with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWpExOXOYlk5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 3-fold cross-validation\n",
        "cv_scores = cross_val_score(sgd_clf, X_train, y_train_2, cv=3, scoring=\"accuracy\")\n",
        "print(\"Cross-validation accuracy scores:\", cv_scores)\n",
        "print(\"Mean accuracy:\", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYLg5T1fYlk5"
      },
      "source": [
        "## Step 6: Generate Confusion Matrix on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m8d0qbzYlk5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Make predictions on test set\n",
        "y_test_2_pred = sgd_clf.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test_2, y_test_2_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nFormat: [[TN, FP],\")\n",
        "print(\"         [FN, TP]]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar-H9qYhYlk5"
      },
      "source": [
        "## Homework Question 1 (3pt): Calculate Precision, Recall, and F1-Score\n",
        "\n",
        "### Part 1: Calculate metrics using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lteqa8PdYlk5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate metrics using sklearn\n",
        "precision = precision_score(y_test_2, y_test_2_pred)\n",
        "recall = recall_score(y_test_2, y_test_2_pred)\n",
        "f1 = f1_score(y_test_2, y_test_2_pred)\n",
        "\n",
        "print(\"Metrics calculated using sklearn.metrics:\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjJfTLi9Ylk5"
      },
      "source": [
        "### Part 2: Validate using formulas from lecture notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH7h9-TjYlk5"
      },
      "outputs": [],
      "source": [
        "# Extract values from confusion matrix\n",
        "TN, FP, FN, TP = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "\n",
        "print(\"Confusion Matrix Components:\")\n",
        "print(f\"True Negatives (TN): {TN}\")\n",
        "print(f\"False Positives (FP): {FP}\")\n",
        "print(f\"False Negatives (FN): {FN}\")\n",
        "print(f\"True Positives (TP): {TP}\")\n",
        "print()\n",
        "\n",
        "# Calculate metrics manually using formulas\n",
        "precision_manual = TP / (TP + FP)\n",
        "recall_manual = TP / (TP + FN)\n",
        "f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual)\n",
        "\n",
        "print(\"Metrics calculated using formulas:\")\n",
        "print(f\"Precision = TP / (TP + FP) = {TP} / ({TP} + {FP}) = {precision_manual:.4f}\")\n",
        "print(f\"Recall = TP / (TP + FN) = {TP} / ({TP} + {FN}) = {recall_manual:.4f}\")\n",
        "print(f\"F1-Score = 2 × (Precision × Recall) / (Precision + Recall) = {f1_manual:.4f}\")\n",
        "print()\n",
        "\n",
        "# Verify that manual calculations match sklearn results\n",
        "print(\"Verification:\")\n",
        "print(f\"Precision match: {np.isclose(precision, precision_manual)}\")\n",
        "print(f\"Recall match: {np.isclose(recall, recall_manual)}\")\n",
        "print(f\"F1-Score match: {np.isclose(f1, f1_manual)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PeXz8jvYlk6"
      },
      "source": [
        "Q1:\n",
        "\n",
        "The three evaluation metrics for the binary classifier (digit 2 vs not 2) are:\n",
        "\n",
        "**1. Precision:** This metric measures the accuracy of positive predictions. It tells us what proportion of images predicted as \"2\" are actually \"2\". The formula is Precision = TP / (TP + FP), where TP represents true positives and FP represents false positives. A high precision means the classifier rarely misclassifies non-2 digits as 2.\n",
        "\n",
        "**2. Recall (Sensitivity):** This metric measures the classifier's ability to find all positive instances. It tells us what proportion of actual \"2\" digits were correctly identified. The formula is Recall = TP / (TP + FN), where FN represents false negatives. A high recall means the classifier rarely misses actual 2 digits.\n",
        "\n",
        "**3. F1-Score:** This is the harmonic mean of precision and recall, providing a single score that balances both metrics. The formula is F1 = 2 × (Precision × Recall) / (Precision + Recall). This metric is particularly useful when you need to balance the trade-off between precision and recall.\n",
        "\n",
        "As we can see, the manual calculations perfectly match the sklearn.metrics results, validating our implementation and understanding of these evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-xnDqlIYlk6"
      },
      "source": [
        "## Step 7: Precision-Recall Trade-off Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDrqck77Ylk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Get decision scores using cross-validation\n",
        "y_scores = cross_val_predict(sgd_clf, X_train, y_train_2, cv=3,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "# Calculate precision and recall for different thresholds\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_2, y_scores)\n",
        "\n",
        "# Plot the trade-off\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
        "plt.axis([-20000, 20000, 0, 1])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.title(\"Precision-Recall Trade-off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_5r1oSDYlk6"
      },
      "source": [
        "## Step 8: ROC Curve Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdHU--DFYlk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_train_2, y_scores)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
        "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.grid()\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.legend(loc=\"lower right\", fontsize=13)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmB_JzLrYlk6"
      },
      "source": [
        "## Step 9: Calculate AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_8benSYlk6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Calculate Area Under the ROC Curve\n",
        "auc_score = roc_auc_score(y_train_2, y_scores)\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n",
        "print(f\"\\nThe AUC score of {auc_score:.4f} indicates strong classifier performance.\")\n",
        "print(\"Values closer to 1.0 represent better classification ability.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKjp0Y4ZYlk7"
      },
      "source": [
        "## Step 10: Multiclass Classification - Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGxmsgt0Ylk7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Scale features to [0, 1] range\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\n",
        "X_test_scaled = scaler.transform(X_test.astype(\"float64\"))\n",
        "\n",
        "print(\"Feature scaling completed.\")\n",
        "print(f\"Original feature range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
        "print(f\"Scaled feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yOPk8MMYlk7"
      },
      "source": [
        "## Step 11: Multiclass Classification with SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF5YAN-LYlk7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Train multiclass SGD classifier\n",
        "sgd_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_test_pred = sgd_clf.predict(X_test_scaled)\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.rc('font', size=9)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n",
        "plt.title(\"Confusion Matrix - SGD Classifier (Multiclass)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvHW9DOcYlk7"
      },
      "source": [
        "## Step 12: Normalized Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6aWZjzXYlk7"
      },
      "outputs": [],
      "source": [
        "# Display normalized confusion matrix (percentages)\n",
        "plt.rc('font', size=10)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n",
        "                                        normalize=\"true\", values_format=\".0%\")\n",
        "plt.title(\"Normalized Confusion Matrix - SGD Classifier\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5CFY8Y4Ylk7"
      },
      "source": [
        "## Step 13: Error Analysis - Weighted Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY--JkstYlk7"
      },
      "outputs": [],
      "source": [
        "# Create weights for misclassified instances only\n",
        "sample_weight = (y_test_pred != y_test)\n",
        "\n",
        "# Display weighted confusion matrix focusing on errors\n",
        "plt.rc('font', size=10)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred,\n",
        "                                        sample_weight=sample_weight,\n",
        "                                        normalize=\"true\", values_format=\".0%\")\n",
        "plt.title(\"Weighted Confusion Matrix - Errors Only\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECFvMcM6Ylk7"
      },
      "source": [
        "## Extension: Ridge Classifier for Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Bf7b0xYlk7"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Train Ridge Classifier with optimized hyperparameters\n",
        "ridge_clf = RidgeClassifier(alpha=0.5, random_state=46)\n",
        "ridge_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_test_pred_ridge = ridge_clf.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "ridge_accuracy = accuracy_score(y_test, y_test_pred_ridge)\n",
        "sgd_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(f\"Ridge Classifier Accuracy: {ridge_accuracy:.4f}\")\n",
        "print(f\"SGD Classifier Accuracy: {sgd_accuracy:.4f}\")\n",
        "print(f\"\\nImprovement: {(ridge_accuracy - sgd_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZYTH8_ZYlk8"
      },
      "source": [
        "## Ridge Classifier Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K24ZcJJYlk8"
      },
      "outputs": [],
      "source": [
        "# Display normalized confusion matrix for Ridge Classifier\n",
        "plt.rc('font', size=10)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_ridge,\n",
        "                                        normalize=\"true\", values_format=\".0%\")\n",
        "plt.title(\"Normalized Confusion Matrix - Ridge Classifier\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}